{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Base MAB Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB(ABC):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_arms: int\n",
    "        Number of arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        \n",
    "    @abstractmethod\n",
    "    def play(self, context=None):\n",
    "        \"\"\"        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        context: float numpy.ndarray, shape = (n_arms, n_dims)\n",
    "            An array of context vectors.\n",
    "            e-greedy and UCB bandits have context = None. \n",
    "        Returns\n",
    "        -------\n",
    "        arm: int\n",
    "            Index of arm.\n",
    "        \"\"\"\n",
    "        self.context = context\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, arm, reward, context):\n",
    "        \"\"\"        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arm: int\n",
    "            Index of arm.\n",
    "        \n",
    "        reward: float\n",
    "            Reward received from the arm.\n",
    "        \n",
    "        context: float numpy.ndarray, shape = (n_arms, n_dims)\n",
    "            An array of context vectors.\n",
    "            e-greedy and UCB bandits have context = None. \n",
    "        \"\"\"\n",
    "        # Update parameters\n",
    "        self.arm = arm\n",
    "        self.reward = reward\n",
    "        self.context = context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-contextual MABs\n",
    "### I. ε-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedy(MAB):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_arms: int\n",
    "        Number of arms\n",
    "\n",
    "    epsilon: float\n",
    "        Explore probability. Must be in the interval [0, 1].\n",
    "    \"\"\"\n",
    "    # initialise values and raise input errors\n",
    "    def __init__(self, num_arms, epsilon):  \n",
    "        super().__init__(num_arms)\n",
    "        self.epsilon = epsilon\n",
    "        self.avg = np.ones(self.num_arms)*np.inf  # mean observed reward of each arm\n",
    "        self.rewards = np.zeros(num_arms)  # cummulative reward of each arm\n",
    "        self.T = np.zeros(num_arms)   # number of times each arm is chosen\n",
    "    \n",
    "    # ε-greedy policy\n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            arm = np.random.randint(0, self.num_arms)\n",
    "        else:\n",
    "            arm = np.random.choice(np.where(self.avg==max(self.avg))[0])  # random tie-breaking\n",
    "        return arm\n",
    "    \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        self.T[arm] += 1\n",
    "        self.rewards[arm] += self.reward\n",
    "        self.avg[arm] = self.rewards[arm]/self.T[arm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Upper Confidence Bound (UCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(MAB):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_arms: int\n",
    "        Number of arms.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms):\n",
    "        super().__init__(num_arms)\n",
    "        self.index = np.ones(self.num_arms)*np.inf   # index values of each arm\n",
    "        self.rewards = np.zeros(num_arms)  # cummulative reward of each arm\n",
    "        self.avg_rewards = np.zeros(num_arms)  # mean reward of each arm, used in UCB index calculation\n",
    "        self.T = np.zeros(num_arms)  # number of times each arm is chosen\n",
    "        self.round = 0  # number of played steps\n",
    "    \n",
    "    def play(self, context=None):\n",
    "        super().play(context)\n",
    "        self.round += 1\n",
    "        self.index = np.where(self.T != 0, self.avg_rewards + np.sqrt(np.log(self.round)/self.T), self.index)  # update index value if self.T[arm] = 0, else stay the same\n",
    "        arm = np.random.choice(np.where(self.index==max(self.index))[0])  # random tie-breaking\n",
    "        return arm\n",
    "        \n",
    "    def update(self, arm, reward, context=None):\n",
    "        super().update(arm, reward, context)\n",
    "        self.T[arm] += 1\n",
    "        self.rewards[arm] += self.reward\n",
    "        self.avg_rewards[arm] = self.rewards[arm]/self.T[arm]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual MABs\n",
    "### I. Linear UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB(MAB):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    dim : int\n",
    "        Number of features for each arm's context.\n",
    "\n",
    "    a : float\n",
    "        alpha explore-exploit parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_arms, dim, a):\n",
    "        super().__init__(num_arms) \n",
    "        self.dim = dim\n",
    "        self.alpha = a\n",
    "        self.posdist = np.zeros(dim)\n",
    "\n",
    "        self.A = np.array(np.identity(dim))\n",
    "        self.inv_A = [inv(self.A)]*10\n",
    "        self.A  = [self.A]*10\n",
    "\n",
    "        self.b = [np.zeros(dim)]*10\n",
    "        self.theta = [(inv(np.identity(dim))@np.zeros(dim))]*10\n",
    "         \n",
    "    def play(self, context):\n",
    "        super().play(context)\n",
    "        for arm in range(self.num_arms):\n",
    "            inv_A = self.inv_A[arm]\n",
    "            theta = self.theta[arm]\n",
    "            # calculate posterior distribution of the coefficient vector\n",
    "            self.posdist[arm] = theta@context[arm] + self.alpha*np.sqrt(context[arm].T@inv_A@context[arm])\n",
    "            \n",
    "        arm = np.random.choice(np.where(self.posdist==max(self.posdist))[0])\n",
    "        return arm\n",
    "    \n",
    "    # update dictionary\n",
    "    def update(self, arm, reward, context):\n",
    "        super().update(arm, reward, context)\n",
    "        reshaped_context = context[arm].reshape(-1,1)\n",
    "        self.A[arm] = self.A[arm] + reshaped_context @ reshaped_context.T\n",
    "        self.inv_A[arm] = inv(self.A[arm])\n",
    "        self.b[arm] = self.b[arm] + reward * context[arm]\n",
    "        self.theta[arm] = self.inv_A[arm] @ self.b[arm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n",
    "The dataset `dataset.txt` contains 10,000 instances of users and news articles. Each instance comprises 102 space-delimited columns of integers:\n",
    " - Column 1: The arm played by a uniformly-random policy.\n",
    " - Column 2: The reward received from the arm played.\n",
    " - Columns 3-102: 100-dim flattened context vectors; 10 features per arm for 10 arms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arms is: 10\n",
      "Number of contexts for each arm is: 10\n",
      "Context vectors of 10 arms for the first instance is:\n",
      "[[ 5.  0.  0. 37.  6.  0.  0.  0.  0. 25.]\n",
      " [ 0.  0.  7.  1.  0.  0.  0. 13.  2.  0.]\n",
      " [ 0.  8.  0.  0.  0. 15. 29.  1.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  3.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  4.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  5.  2.  7.  0.  0.  0.  0. 11.  0.]\n",
      " [ 5.  0.  0.  0.  0.  0.  0.  3.  2.  0.]\n",
      " [ 0.  0.  0.  1.  2. 47.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  3.  0.  0. 17. 30.  4.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# load dataset and initialize arms, rewards and contexts vectors\n",
    "data = np.loadtxt(\"dataset.txt\")\n",
    "chosen_arms = data[:,0]\n",
    "rewards = data[:,1]\n",
    "contexts = data[:,2:]\n",
    "\n",
    "chosen_arms = chosen_arms.astype(int)\n",
    "rewards = rewards.astype(float)\n",
    "contexts = contexts.astype(float)\n",
    "num_arms = len(np.unique(chosen_arms))\n",
    "print(\"Number of arms is:\", num_arms)\n",
    "\n",
    "num_events = len(contexts)\n",
    "dim = int(len(contexts[0])/num_arms)\n",
    "print(\"Number of contexts for each arm is:\", dim)\n",
    "contexts = contexts.reshape(num_events, num_arms, dim)\n",
    "print(\"Context vectors of 10 arms for the first instance is:\")\n",
    "print(contexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Off-policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPE(mab, arms, rewards, contexts, n_rounds=1000):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mab: MAB \n",
    "        MAB instance to evaluate.\n",
    "    \n",
    "    arms: int numpy.ndarray\n",
    "        Array containing the history of pulled arms.\n",
    "    \n",
    "    rewards: float numpy.ndarray\n",
    "        Array containing the history of rewards.\n",
    "    \n",
    "    contexts: float numpy.ndarray\n",
    "        History of contexts presented to arms. \n",
    "        The 0-th axis indexes the events in the history.\n",
    "        Each event consists of 10 context vectors for the arms.\n",
    "        \n",
    "    n_rounds: int\n",
    "        Desired number of matching arms.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: float numpy.ndarray\n",
    "        Rewards for the matching events.\n",
    "    \"\"\"\n",
    "    t = 0         # current round\n",
    "    R = []        # total reward\n",
    "    history = []  # used historical events\n",
    "    matches_count = 0\n",
    "    n_events = arms.shape[0]\n",
    "    # play until we have n_rounds of matching arms or until we run out of data\n",
    "    for i in range(n_events):\n",
    "        if t == n_rounds:\n",
    "            break\n",
    "        chosen_arm = mab.play(contexts[i])\n",
    "        # update parameters and play only when chosen arm matches arm in data\n",
    "        # print(\"chosen arm: {}, arm: {}\".format(chosen_arm, arms[i]))\n",
    "        if chosen_arm == arms[i]:\n",
    "            matches_count += 1\n",
    "            R.append(rewards[i])\n",
    "            history.append([arms[i], rewards[i], contexts[i]])\n",
    "            mab.update(arms[i], rewards[i], contexts[i])  # for ε-greedy and UCB, contexts = None\n",
    "            t += 1\n",
    "    reward_list = np.array(R)\n",
    "    return reward_list, matches_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy average reward is: 0.2520\n",
      "Number of matches: 1000\n"
     ]
    }
   ],
   "source": [
    "mab = EGreedy(num_arms, 0.1) \n",
    "result_EGreedy, eg_matches_count = OPE(mab, chosen_arms, rewards, contexts,)\n",
    "print('Epsilon-greedy average reward is: %.4f' % np.mean(result_EGreedy))\n",
    "print('Number of matches:', eg_matches_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCB average reward: 0.1571\n"
     ]
    }
   ],
   "source": [
    "mab = UCB(num_arms)\n",
    "result_UCB, ucb_matches_count = OPE(mab, chosen_arms, rewards, contexts,)\n",
    "print('UCB average reward: %.4f' % np.mean(result_UCB))\n",
    "print('Number of matches:', ucb_matches_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinUCB average reward: 0.5633\n",
      "Number of matches: 996\n"
     ]
    }
   ],
   "source": [
    "mab = LinUCB(num_arms, dim, 1.0)\n",
    "result_LinUCB, linucb_matches_count = OPE(mab, chosen_arms, rewards, contexts,)\n",
    "print('LinUCB average reward: %.4f' % np.mean(result_LinUCB))\n",
    "print('Number of matches:', linucb_matches_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparison between MABs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================\n",
    "#    Epsilon Greedy ...\n",
    "#===============================\n",
    "mab = EGreedy(10, 0.05)\n",
    "results_EpsGreedy, chosen_arms_EpsGreedy = OPE(mab, chosen_arms, rewards, contexts)\n",
    "cumulative_reward_EpsGreedy = np.cumsum(results_EpsGreedy)\n",
    "\n",
    "#===============================\n",
    "#    UCB ...\n",
    "#===============================\n",
    "mab = UCB(10)\n",
    "results_UCB, chosen_arms_UCB = OPE(mab, chosen_arms, rewards, contexts)\n",
    "cumulative_reward_UCB = np.cumsum(results_UCB)\n",
    "#===============================\n",
    "#    Linear UCB (Contextual) ...\n",
    "#===============================\n",
    "mab = LinUCB(10, 10, 1.0)\n",
    "results_LinUCB, chosen_arms_LinUCB = OPE(mab, chosen_arms, rewards, contexts)\n",
    "cumulative_reward_LinUCB = np.cumsum(results_LinUCB)\n",
    "#===============================\n",
    "#    Plotting results ...\n",
    "#===============================\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(cumulative_reward_LinUCB/np.linspace(1,800,800), label = r\"$\\alpha=1$ (LinUCB)\")\n",
    "plt.plot(cumulative_reward_UCB/(np.linspace(1,800,800)), label = r\"$\\rho=1$ (UCB)\")\n",
    "plt.plot(cumulative_reward_EpsGreedy/(np.linspace(1,800,800)), label = r\"$\\epsilon=0.05$ (greedy)\")\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "plt.xlabel(\"Rounds\")\n",
    "plt.ylabel(r\"$T^{-1}\\sum_{t=1}^T\\ r_{t,a}$\", fontsize='large')\n",
    "plt.title(\"Per-round Cumulative Rewards after single simulation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
